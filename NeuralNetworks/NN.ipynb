{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "train_raw = pd.read_csv(\"./bank-note/train.csv\", header=None).values\n",
    "train_cols = train_raw.shape[1]\n",
    "train_rows = train_raw.shape[0]\n",
    "train_x = np.copy(train_raw)\n",
    "train_x[:,train_cols - 1] = 1 # augment the bias 1\n",
    "train_y = train_raw[:, train_cols - 1]\n",
    "train_y[train_y > 0] = 1      # map 1 -> 1\n",
    "train_y[train_y == 0] = -1    # map 0 -> -1\n",
    "\n",
    "test_raw = pd.read_csv(\"./bank-note/test.csv\", header=None).values\n",
    "test_cols = test_raw.shape[1]\n",
    "test_rows = test_raw.shape[0]\n",
    "test_x = np.copy(test_raw)\n",
    "test_x[:,test_cols - 1] = 1\n",
    "test_y = test_raw[:, test_cols - 1]\n",
    "test_y[test_y > 0] = 1\n",
    "test_y[test_y == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def loss(y, a):\n",
    "    return 0.5 * (y-a)**2\n",
    "###### forward pass\n",
    "## given an augmented example x(n, 1) and the corresponsing weight matrix W(n1, n) \n",
    "## returns the next level's activations (n1, 1)\n",
    "## does not apply sigmoid if is_last_layer = true\n",
    "def forward_step(x, w, is_last_layer):\n",
    "    activations = np.dot(w,x)\n",
    "    if is_last_layer:\n",
    "        return activations\n",
    "    return [sigmoid(z) for z in activations]\n",
    "\n",
    "## given an augmented example x(n, 1) \n",
    "## and the list of weight matrices W[w0, w1,w2] corresponsing to the weights each layer should be multiplied\n",
    "## returns the final output and activations of the layers\n",
    "def forward_pass(x, W):\n",
    "    a = x\n",
    "    activations = [x]\n",
    "    for layer in range(len(W)-1):\n",
    "        a = forward_step(a, W[layer], False)   # compute the activations of layes[0,n-2]\n",
    "        activations.append(a)\n",
    "    activations.append(forward_step(a, W[len(W)-1], True))       # compute the last layer output\n",
    "    return activations\n",
    "\n",
    "###### backward pass\n",
    "## given an augmented example x(n, 1)\n",
    "## given the true label y\n",
    "## given the list of layers weight matrices W\n",
    "## given list of layer activations\n",
    "## returns DW [Dw0, dw1, ...] that is the derivatives of L with respect to weights of each layer\n",
    "def backward_pass(x,y, W,A):\n",
    "    DA = y - A[len(A)-1] # derivatives L with respect to the activations of this layer, initially DA  = dL/dy = y - y*\n",
    "    DW = []              # list of derivatives with repect to the weights of all layers\n",
    "    for layer in reversed(range(len(W))): #0,1,2\n",
    "#         if layer == (len(W) - 1): ## if the last layer no sigmoid derivation\n",
    "            A = np.reshape(np.array(A[layer]), (1,-1))               # activations of this layer\n",
    "            A_next = np.reshape(np.array(A[layer+1]), (1,-1))        # activations of next layer\n",
    "            t = A_next * (1 - A_next) * np.reshape(DA, (-1,1))       # sigmoid derivative\n",
    "            DW.insert(0,np.matmul(t, a))\n",
    "#             DA = np.matmul(np.transpose(W[layer]),t)\n",
    "            \n",
    "#             DA[-1] = 0 # drop the bias derivative\n",
    "# #         else:\n",
    "# #             DW.insert(0,np.matmul(np.reshape(DA, (-1,1)), np.reshape(np.array(A[layer]), (1,-1))))\n",
    "# #             DA = np.matmul(np.transpose(W[layer]),DA)\n",
    "# #             DA[-1] = 0 # drop the bias derivative\n",
    "    return DW\n",
    "        \n",
    "###### Stocastic Gradient Decent\n",
    "# def SGD(X,Y,learningRate, T):\n",
    "#     rows = X.shape[0]\n",
    "#     cols = X.shape[1]\n",
    "#     w = np.zeros(cols)                              # 1. Initialize w = 0 ∈ ℜn\n",
    "#     indices = np.arange(rows)\n",
    "    \n",
    "#     for epoch in range(T):                          # 2. For epoch = 1 … T:\n",
    "#         np.random.shuffle(indices)                       #1. Shuffle the data\n",
    "#         x = X[indices,:]\n",
    "#         y = Y[indices]\n",
    "#         for i in range(rows):                            #2. For each training example (xi, yi) ∈ D:\n",
    "#             if np.sum(x[i] * w) * y[i] <= 0:                  #If yi wTxi ≤ 0, update w ← w + r yi xi\n",
    "#                 w = w + learningRate * y[i] * x[i]\n",
    "#     return w                                        # 3. Return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-016a850d4f6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m###### run the >> backward_pass << and get the derivations for each layer ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mDW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m# print(DW)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m###########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-5b0409043851>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(x, y, W, A)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#         if layer == (len(W) - 1): ## if the last layer no sigmoid derivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# activations of this layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0ma_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# activations of next layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma_next\u001b[0m\u001b[0;34m)\u001b[0m                       \u001b[0;31m# sigmoid derivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "##### NN Architechture ####\n",
    "## You can add more layers. Also can change the size of each layer\n",
    "layerOneSize = 3\n",
    "layerTwoSize = 2\n",
    "LayerSizes = [train_cols, layerOneSize, layerTwoSize, 1]\n",
    "\n",
    "## randomly initializes the weight matrix of each layer\n",
    "W = [np.random.rand(LayerSizes[layer+1], LayerSizes[layer]) for layer in range(len(LayerSizes)-1)]\n",
    "for w in W:               # set the bias weight columns to 0\n",
    "    w[:,-1 ] = 0\n",
    "###########################\n",
    "\n",
    "###### run the >> forward_pass << and get the activations for each layer ####\n",
    "A = forward_pass(train_x[0], W)\n",
    "# print(activations)\n",
    "###########################\n",
    "\n",
    "###### run the >> backward_pass << and get the derivations for each layer ####\n",
    "DW = backward_pass(train_x[0],train_y[0], W, A)\n",
    "# print(DW)\n",
    "###########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
