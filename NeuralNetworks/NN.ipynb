{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smart-accuracy",
   "metadata": {},
   "source": [
    "# Imports and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "going-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "train_raw = pd.read_csv(\"./bank-note/train.csv\", header=None).values\n",
    "train_cols = train_raw.shape[1]\n",
    "train_rows = train_raw.shape[0]\n",
    "train_x = np.copy(train_raw)\n",
    "train_x = np.delete(np.concatenate([np.ones((train_rows,1)),train_x], axis=1), -1,1) # augment the bias 1\n",
    "train_y = train_raw[:, train_cols - 1]\n",
    "train_y[train_y > 0] = 1      # map 1 -> 1\n",
    "train_y[train_y == 0] = -1    # map 0 -> -1\n",
    "\n",
    "test_raw = pd.read_csv(\"./bank-note/test.csv\", header=None).values\n",
    "test_cols = test_raw.shape[1]\n",
    "test_rows = test_raw.shape[0]\n",
    "test_x = np.copy(test_raw)\n",
    "test_x = np.delete(np.concatenate([np.ones((test_rows,1)),test_x], axis=1), -1,1) # augment the bias 1\n",
    "test_y = test_raw[:, test_cols - 1]\n",
    "test_y[test_y > 0] = 1\n",
    "test_y[test_y == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-library",
   "metadata": {},
   "source": [
    "# NN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "purple-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x, W):\n",
    "    predictions = []\n",
    "    for example in x:\n",
    "        predictions.append(forward_pass(example, W)[-1])\n",
    "    return predictions\n",
    "def get_error(x,y,W):\n",
    "    n_rows = x.shape[0]\n",
    "    predictions = np.sign(pred(x, W))    # predictions = sign(Wt*x)\n",
    "    predictions = np.reshape(predictions,(1,-1))\n",
    "    incorrect_predictions = predictions - y\n",
    "    count_incorrect_predictions = np.count_nonzero(incorrect_predictions)\n",
    "    Error = count_incorrect_predictions/ n_rows\n",
    "    return Error\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def loss(y, a):\n",
    "    return 0.5 * (y-a)**2\n",
    "###### forward pass\n",
    "## given an augmented example x(n, 1) and the corresponsing weight matrix W(n1, n) \n",
    "## returns the next level's activations (n1, 1)\n",
    "## does not apply sigmoid if is_last_layer = true\n",
    "def forward_step(x, w, is_last_layer):\n",
    "    activations = np.reshape(np.dot(np.reshape(x,(1,-1)),w),(-1,1))\n",
    "    if is_last_layer:\n",
    "        return activations\n",
    "    return sigmoid(activations)\n",
    "\n",
    "## given an augmented example x(n, 1) \n",
    "## and the list of weight matrices W[w0, w1,w2] corresponsing to the weights each layer should be multiplied\n",
    "## returns the final output and activations of the layers\n",
    "def forward_pass(x, W):\n",
    "    a = np.reshape(x,(-1,1))\n",
    "    activations = [a]\n",
    "    for layer in range(len(W)-1):\n",
    "        a = forward_step(a, W[layer], False)   # compute the activations of layes[0,n-2]\n",
    "        a[0] = 1                               # set the bias term\n",
    "        activations.append(a)\n",
    "    activations.append(forward_step(a, W[len(W)-1], True))       # compute the last layer output\n",
    "    return activations\n",
    "\n",
    "###### backward pass\n",
    "## given an augmented example x(n, 1)\n",
    "## given the true label y\n",
    "## given the list of layers weight matrices W\n",
    "## given list of layer activations\n",
    "## returns DW [Dw0, dw1, ...] that is the derivatives of L with respect to weights of each layer\n",
    "def backward_pass(x,y, W,A):\n",
    "    DA = A[len(A)-1] - y # derivatives L with respect to the activations of this layer, initially DA  = dL/dy = y - y*\n",
    "    DW = []              # list of derivatives with repect to the weights of all layers\n",
    "    for layer in reversed(range(len(W))): #0,1,2\n",
    "        a = A[layer]               # activations of this layer\n",
    "        a_next = A[layer+1]        # activations of next layer \n",
    "        t = 0\n",
    "        if layer == (len(W) - 1):  # if the last layer no sigmoid derivation\n",
    "            t = np.reshape(DA,(-1,1))\n",
    "        else:\n",
    "            t = np.reshape(a_next * (1 - a_next) * DA,(-1,1))\n",
    "\n",
    "        DW.insert(0,np.matmul(a,np.reshape(t, (1,-1))))\n",
    "        if layer != 0:\n",
    "            DA = np.matmul(W[layer], t)\n",
    "            DA[0] = 0                 # drop the derivation with respect to bias\n",
    "    return DW\n",
    "        \n",
    "##### Stocastic Gradient Decent\n",
    "def SGD(X,Y, W,learningRate, alpha, T):\n",
    "    rows = X.shape[0]\n",
    "    cols = X.shape[1]\n",
    "    indices = np.arange(rows)\n",
    "    for epoch in range(T):                          # 2. For epoch = 1 … T:\n",
    "        np.random.shuffle(indices)                       #1. Shuffle the data\n",
    "        x = X[indices,:]\n",
    "        y = Y[indices]\n",
    "        r = learningRate / (1 + learningRate/alpha * epoch)\n",
    "        for i in range(rows):                            #2. For each training example (xi, yi) ∈ D:\n",
    "            A  = forward_pass(x[i], W)                       # compute Activations\n",
    "            DW = backward_pass(x[i],y[i], W, A)              # compute Weight Gradients\n",
    "            for i in range (len(W)):\n",
    "                W[i] = W[i] - r*DW[i]                        # update W <-- W - rDW\n",
    "        \n",
    "    return W                                        # 3. Return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-maintenance",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "atomic-renaissance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "question 2.a\n",
      "This is to demonstrate forward and backward are working:\n",
      "[[ 0 -1  1]\n",
      " [ 0 -2  2]\n",
      " [ 0 -3  3]]\n",
      "=====\n",
      "[[ 0 -1  1]\n",
      " [ 0 -2  2]\n",
      " [ 0 -3  3]]\n",
      "=====\n",
      "[[-1. ]\n",
      " [ 2. ]\n",
      " [-1.5]]\n",
      "=====\n",
      "Forward-Pass Activations:\n",
      "[array([[1],\n",
      "       [1],\n",
      "       [1]]), array([[1.        ],\n",
      "       [0.00247262],\n",
      "       [0.99752738]]), array([[1.        ],\n",
      "       [0.01802994],\n",
      "       [0.98197006]]), array([[-2.43689523]])]\n",
      "=====\n",
      "Backward-Pass Gradients of Weight matrices:\n",
      "[array([[0.        , 0.00105061, 0.00157591],\n",
      "       [0.        , 0.00105061, 0.00157591],\n",
      "       [0.        , 0.00105061, 0.00157591]]), array([[ 0.        , -0.12169947,  0.09127461],\n",
      "       [ 0.        , -0.00030092,  0.00022569],\n",
      "       [ 0.        , -0.12139856,  0.09104892]]), array([[-3.43689523],\n",
      "       [-0.061967  ],\n",
      "       [-3.37492823]])]\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "print(\"###########################\")\n",
    "print(\"question 2.a\")\n",
    "print(\"This is to demonstrate forward and backward are working:\")\n",
    "##### W just for debug\n",
    "w1 = np.array(\\\n",
    "             [\\\n",
    "              [0,-1,1],\\\n",
    "              [0,-2,2],\\\n",
    "              [0,-3,3]\\\n",
    "             ]\\\n",
    "            )\n",
    "w2 = np.array(\\\n",
    "             [\\\n",
    "              [0,-1,1],\\\n",
    "              [0,-2,2],\\\n",
    "              [0,-3,3]\\\n",
    "             ]\\\n",
    "             )\n",
    "w3 = np.array(\\\n",
    "             [\\\n",
    "              [-1],\\\n",
    "              [2],\\\n",
    "              [-1.5]\\\n",
    "             ]\\\n",
    "             )\n",
    "W = [w1,w2,w3]\n",
    "for w in W:\n",
    "    print (w)\n",
    "    print(\"=====\")\n",
    "\n",
    "###########################\n",
    "## testing the Forward and Backward passes\n",
    "#   run the >> Forward Pass << and get the activations for each layer \n",
    "x0 = [1,1,1]\n",
    "y0 = 1\n",
    "print(\"Forward-Pass Activations:\")\n",
    "A = forward_pass(x0, W)\n",
    "print(A)\n",
    "print(\"=====\")\n",
    "\n",
    "#   run the >> Backward Pass << and get the derivations for each layer ####\n",
    "print(\"Backward-Pass Gradients of Weight matrices:\")\n",
    "DW = backward_pass(x0,y0, W, A)\n",
    "print(DW)\n",
    "print(\"=====\")\n",
    "###########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aquatic-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "question 2.b\n",
      "For  5 nodes the Train Error is: 0.008027522935779817\n",
      "For  5 nodes the Test Error is: 0.008\n",
      "=====\n",
      "For  10 nodes the Train Error is: 0.0\n",
      "For  10 nodes the Test Error is: 0.0\n",
      "=====\n",
      "For  25 nodes the Train Error is: 0.0\n",
      "For  25 nodes the Test Error is: 0.0\n",
      "=====\n",
      "For  50 nodes the Train Error is: 0.0\n",
      "For  50 nodes the Test Error is: 0.002\n",
      "=====\n",
      "For  100 nodes the Train Error is: 0.0\n",
      "For  100 nodes the Test Error is: 0.0\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "print(\"###########################\")\n",
    "print(\"question 2.b\")\n",
    "for nodes in [5,10,25,50,100]:\n",
    "    layerOneSize = nodes\n",
    "    layerTwoSize = nodes\n",
    "    LayerSizes = [train_cols, layerOneSize, layerTwoSize, 1]\n",
    "\n",
    "    # randomly initializes the weight matrix of each layer\n",
    "    W = [np.random.randn(LayerSizes[layer], LayerSizes[layer+1]) for layer in range(len(LayerSizes)-1)]\n",
    "    for i in range(len(W)-1):               # set the bias weight columns to 0 (except for output layer)\n",
    "        W[i][:,0 ] = 0\n",
    "    W = SGD(train_x, train_y, W, 0.05,0.1, 30)\n",
    "    print(\"For \",nodes,\"nodes the Train Error is:\",get_error(train_x, train_y, W))\n",
    "    print(\"For \",nodes,\"nodes the Test Error is:\",get_error(test_x, test_y, W))\n",
    "    print(\"=====\")\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adjusted-manchester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "question 2.c\n",
      "For  5 nodes the Train Error is: 0.009174311926605505\n",
      "For  5 nodes the Test Error is: 0.01\n",
      "=====\n",
      "For  10 nodes the Train Error is: 0.009174311926605505\n",
      "For  10 nodes the Test Error is: 0.008\n",
      "=====\n",
      "For  25 nodes the Train Error is: 0.009174311926605505\n",
      "For  25 nodes the Test Error is: 0.01\n",
      "=====\n",
      "For  50 nodes the Train Error is: 0.017201834862385322\n",
      "For  50 nodes the Test Error is: 0.016\n",
      "=====\n",
      "For  100 nodes the Train Error is: 0.011467889908256881\n",
      "For  100 nodes the Test Error is: 0.012\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "print(\"###########################\")\n",
    "print(\"question 2.c\")\n",
    "for nodes in [5,10,25,50,100]:\n",
    "    layerOneSize = nodes\n",
    "    layerTwoSize = nodes\n",
    "    LayerSizes = [train_cols, layerOneSize, layerTwoSize, 1]\n",
    "\n",
    "    # randomly initializes the weight matrix of each layer\n",
    "    W = [np.zeros((LayerSizes[layer], LayerSizes[layer+1])) for layer in range(len(LayerSizes)-1)]\n",
    "    for i in range(len(W)-1):               # set the bias weight columns to 0 (except for output layer)\n",
    "        W[i][:,0 ] = 0\n",
    "    W = SGD(train_x, train_y, W, 0.05,0.1, 30)\n",
    "    print(\"For \",nodes,\"nodes the Train Error is:\",get_error(train_x, train_y, W))\n",
    "    print(\"For \",nodes,\"nodes the Test Error is:\",get_error(test_x, test_y, W))\n",
    "    print(\"=====\")\n",
    "###########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
