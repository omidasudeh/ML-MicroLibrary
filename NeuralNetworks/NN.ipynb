{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smart-accuracy",
   "metadata": {},
   "source": [
    "# Imports and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "going-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "train_raw = pd.read_csv(\"./bank-note/train.csv\", header=None).values\n",
    "train_cols = train_raw.shape[1]\n",
    "train_rows = train_raw.shape[0]\n",
    "train_x = np.copy(train_raw)\n",
    "train_x = np.delete(np.concatenate([np.ones((train_rows,1)),train_x], axis=1), -1,1) # augment the bias 1\n",
    "train_y = train_raw[:, train_cols - 1]\n",
    "train_y[train_y > 0] = 1      # map 1 -> 1\n",
    "train_y[train_y == 0] = -1    # map 0 -> -1\n",
    "\n",
    "test_raw = pd.read_csv(\"./bank-note/test.csv\", header=None).values\n",
    "test_cols = test_raw.shape[1]\n",
    "test_rows = test_raw.shape[0]\n",
    "test_x = np.copy(test_raw)\n",
    "test_x = np.delete(np.concatenate([np.ones((test_rows,1)),test_x], axis=1), -1,1) # augment the bias 1\n",
    "test_y = test_raw[:, test_cols - 1]\n",
    "test_y[test_y > 0] = 1\n",
    "test_y[test_y == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-library",
   "metadata": {},
   "source": [
    "# NN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "purple-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x, W):\n",
    "    predictions = []\n",
    "    for example in x:\n",
    "        predictions.append(forward_pass(example, W)[-1])\n",
    "    return predictions\n",
    "def get_error(x,y,W):\n",
    "    n_rows = x.shape[0]\n",
    "    predictions = np.sign(pred(x, W))    # predictions = sign(Wt*x)\n",
    "    predictions = np.reshape(predictions,(1,-1))\n",
    "    incorrect_predictions = predictions - y\n",
    "    count_incorrect_predictions = np.count_nonzero(incorrect_predictions)\n",
    "    Error = count_incorrect_predictions/ n_rows\n",
    "    return Error\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def loss(y, a):\n",
    "    return 0.5 * (y-a)**2\n",
    "###### forward pass\n",
    "## given an augmented example x(n, 1) and the corresponsing weight matrix W(n1, n) \n",
    "## returns the next level's activations (n1, 1)\n",
    "## does not apply sigmoid if is_last_layer = true\n",
    "def forward_step(x, w, is_last_layer):\n",
    "    activations = np.reshape(np.dot(np.reshape(x,(1,-1)),w),(-1,1))\n",
    "    if is_last_layer:\n",
    "        return activations\n",
    "    return sigmoid(activations)\n",
    "\n",
    "## given an augmented example x(n, 1) \n",
    "## and the list of weight matrices W[w0, w1,w2] corresponsing to the weights each layer should be multiplied\n",
    "## returns the final output and activations of the layers\n",
    "def forward_pass(x, W):\n",
    "    a = np.reshape(x,(-1,1))\n",
    "    activations = [a]\n",
    "    for layer in range(len(W)-1):\n",
    "        a = forward_step(a, W[layer], False)   # compute the activations of layes[0,n-2]\n",
    "        a[0] = 1                               # set the bias term\n",
    "        activations.append(a)\n",
    "    activations.append(forward_step(a, W[len(W)-1], True))       # compute the last layer output\n",
    "    return activations\n",
    "\n",
    "###### backward pass\n",
    "## given an augmented example x(n, 1)\n",
    "## given the true label y\n",
    "## given the list of layers weight matrices W\n",
    "## given list of layer activations\n",
    "## returns DW [Dw0, dw1, ...] that is the derivatives of L with respect to weights of each layer\n",
    "def backward_pass(x,y, W,A):\n",
    "    DA = A[len(A)-1] - y # derivatives L with respect to the activations of this layer, initially DA  = dL/dy = y - y*\n",
    "    DW = []              # list of derivatives with repect to the weights of all layers\n",
    "    for layer in reversed(range(len(W))): #0,1,2\n",
    "#         print(\"layer:\",layer)\n",
    "        a = A[layer]               # activations of this layer\n",
    "#         print(\"shape a:\",a.shape)\n",
    "\n",
    "        a_next = A[layer+1]        # activations of next layer \n",
    "#         print(\"shape a_next:\",a_next.shape)\n",
    "#         print(\"shape DA:\", DA.shape)\n",
    "        t = 0\n",
    "        if layer == (len(W) - 1):  # if the last layer no sigmoid derivation\n",
    "            t = np.reshape(DA,(-1,1))\n",
    "        else:\n",
    "            t = np.reshape(a_next * (1 - a_next) * DA,(-1,1))\n",
    "#         print(\"shape t:\",t.shape)\n",
    "\n",
    "        DW.insert(0,np.matmul(a,np.reshape(t, (1,-1))))\n",
    "#         print(\"W[\",layer,\"].shape:\", W[layer].shape)\n",
    "#         print(\"DW:\")\n",
    "#         print(DW)\n",
    "        \n",
    "        if layer != 0:\n",
    "            DA = np.matmul(W[layer], t)\n",
    "            DA[0] = 0                 # drop the derivation with respect to bias \n",
    "#             print(\"DA:\")\n",
    "#             print(DA)\n",
    "#         print(\"=====\")\n",
    "    return DW\n",
    "        \n",
    "##### Stocastic Gradient Decent\n",
    "def SGD(X,Y, W,learningRate, alpha, T):\n",
    "    rows = X.shape[0]\n",
    "    cols = X.shape[1]\n",
    "    indices = np.arange(rows)\n",
    "    for epoch in range(T):                          # 2. For epoch = 1 … T:\n",
    "        np.random.shuffle(indices)                       #1. Shuffle the data\n",
    "        x = X[indices,:]\n",
    "        y = Y[indices]\n",
    "        r = learningRate / (1 + learningRate/alpha * epoch)\n",
    "        for i in range(rows):                            #2. For each training example (xi, yi) ∈ D:\n",
    "            A  = forward_pass(x[i], W)                       # compute Activations\n",
    "            DW = backward_pass(x[i],y[i], W, A)              # compute Weight Gradients\n",
    "            for i in range (len(W)):\n",
    "                W[i] = W[i] - r*DW[i]                        # update W <-- W - rDW\n",
    "#             print(Loss(W, x))\n",
    "        \n",
    "    return W                                        # 3. Return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-maintenance",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aquatic-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward-Pass Activations:\n",
      "[array([[ 1.    ],\n",
      "       [ 3.8481],\n",
      "       [10.1539],\n",
      "       [-3.8561],\n",
      "       [-4.2228]]), array([[1.        ],\n",
      "       [0.00737354],\n",
      "       [0.99999977]]), array([[1.        ],\n",
      "       [0.06460359],\n",
      "       [0.5351836 ]]), array([[-0.5311976]])]\n",
      "=====\n",
      "Backward-Pass Gradients of Weight matrices:\n",
      "[array([[ 0.00000000e+00,  7.66217840e-05,  1.04428306e-08],\n",
      "       [ 0.00000000e+00,  2.94848287e-04,  4.01850563e-08],\n",
      "       [ 0.00000000e+00,  7.78009932e-04,  1.06035457e-07],\n",
      "       [ 0.00000000e+00, -2.95461261e-04, -4.02685990e-08],\n",
      "       [ 0.00000000e+00, -3.23558469e-04, -4.40979849e-08]]), array([[ 0.00000000e+00, -7.07243453e-03, -7.12627708e-02],\n",
      "       [ 0.00000000e+00, -5.21488598e-05, -5.25458699e-04],\n",
      "       [ 0.00000000e+00, -7.07243292e-03, -7.12627546e-02]]), array([[0.4688024 ],\n",
      "       [0.03028632],\n",
      "       [0.25089536]])]\n",
      "=====\n",
      "For  5 nodes the Train Error is: 0.0\n",
      "For  5 nodes the Test Error is: 0.0\n",
      "=====\n",
      "For  10 nodes the Train Error is: 0.0\n",
      "For  10 nodes the Test Error is: 0.004\n",
      "=====\n",
      "For  25 nodes the Train Error is: 0.0011467889908256881\n",
      "For  25 nodes the Test Error is: 0.002\n",
      "=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-ac9ff01a49b6>:16: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "<ipython-input-18-ac9ff01a49b6>:73: RuntimeWarning: overflow encountered in matmul\n",
      "  DA = np.matmul(W[layer], t)\n",
      "<ipython-input-18-ac9ff01a49b6>:64: RuntimeWarning: invalid value encountered in multiply\n",
      "  t = np.reshape(a_next * (1 - a_next) * DA,(-1,1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  50 nodes the Train Error is: 1.0\n",
      "For  50 nodes the Test Error is: 1.0\n",
      "=====\n",
      "For  100 nodes the Train Error is: 1.0\n",
      "For  100 nodes the Test Error is: 1.0\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "## question 2.a\n",
    "## NN Architechture ####\n",
    "# You can add more layers. Also can change the size of each layer\n",
    "layerOneSize = 3\n",
    "layerTwoSize = 3\n",
    "LayerSizes = [train_cols, layerOneSize, layerTwoSize, 1]\n",
    "\n",
    "# randomly initializes the weight matrix of each layer\n",
    "W = [np.random.randn(LayerSizes[layer], LayerSizes[layer+1]) for layer in range(len(LayerSizes)-1)]\n",
    "for i in range(len(W)-1):               # set the bias weight columns to 0 (except for output layer)\n",
    "    W[i][:,0 ] = 0\n",
    "\n",
    "###### W just for debug\n",
    "# w1 = np.array(\\\n",
    "#              [\\\n",
    "#               [0,-1,1],\\\n",
    "#               [0,-2,2],\\\n",
    "#               [0,-3,3]\\\n",
    "#              ]\\\n",
    "#             )\n",
    "# w2 = np.array(\\\n",
    "#              [\\\n",
    "#               [0,-1,1],\\\n",
    "#               [0,-2,2],\\\n",
    "#               [0,-3,3]\\\n",
    "#              ]\\\n",
    "#              )\n",
    "# w3 = np.array(\\\n",
    "#              [\\\n",
    "#               [-1],\\\n",
    "#               [2],\\\n",
    "#               [-1.5]\\\n",
    "#              ]\\\n",
    "#              )\n",
    "# W = [w1,w2,w3]\n",
    "# for w in W:\n",
    "#     print (w)\n",
    "#     print(\"=====\")\n",
    "\n",
    "###########################\n",
    "\n",
    "\n",
    "###########################\n",
    "## testing the Forward and Backward passes\n",
    "#   run the >> Forward Pass << and get the activations for each layer \n",
    "print(\"Forward-Pass Activations:\")\n",
    "A = forward_pass(train_x[0], W)\n",
    "print(A)\n",
    "print(\"=====\")\n",
    "\n",
    "#   run the >> Backward Pass << and get the derivations for each layer ####\n",
    "print(\"Backward-Pass Gradients of Weight matrices:\")\n",
    "DW = backward_pass(train_x[0],train_y[0], W, A)\n",
    "print(DW)\n",
    "print(\"=====\")\n",
    "###########################\n",
    "\n",
    "###########################\n",
    "## question 2.b\n",
    "for nodes in [5,10,25,50,100]:\n",
    "    layerOneSize = nodes\n",
    "    layerTwoSize = nodes\n",
    "    LayerSizes = [train_cols, layerOneSize, layerTwoSize, 1]\n",
    "\n",
    "    # randomly initializes the weight matrix of each layer\n",
    "    W = [np.random.randn(LayerSizes[layer], LayerSizes[layer+1]) for layer in range(len(LayerSizes)-1)]\n",
    "    for i in range(len(W)-1):               # set the bias weight columns to 0 (except for output layer)\n",
    "        W[i][:,0 ] = 0\n",
    "    W = SGD(train_x, train_y, W, 0.5,0.1, 30)\n",
    "    print(\"For \",nodes,\"nodes the Train Error is:\",get_error(train_x, train_y, W))\n",
    "    print(\"For \",nodes,\"nodes the Test Error is:\",get_error(test_x, test_y, W))\n",
    "    print(\"=====\")\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-politics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "recorded-imaging",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-internship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
