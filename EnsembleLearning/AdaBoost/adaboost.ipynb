{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy as cp\n",
    "test_df = pd.read_csv(\"./bank/test.csv\", header=None)\n",
    "m_test = len(test_df)\n",
    "\n",
    "train_df = pd.read_csv(\"./bank/train.csv\", header=None)\n",
    "m_train = len(train_df)\n",
    "\n",
    "# train_df = pd.read_csv(\"./bank/sample1.csv\", header=None)\n",
    "# m_train = len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_sample = train_df.sample(30)\n",
    "# print(train_df_sample)\n",
    "# train_df_sample.to_csv(\"sample.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing numerical attribute: age threashold: 38.0\n",
      "preprocessing numerical attribute: balance threashold: 452.5\n",
      "preprocessing numerical attribute: day threashold: 16.0\n",
      "preprocessing numerical attribute: duration threashold: 180.0\n",
      "preprocessing numerical attribute: campaign threashold: 2.0\n",
      "preprocessing numerical attribute: pdays threashold: -1.0\n",
      "preprocessing numerical attribute: previous threashold: 0.0\n",
      "preprocessing numerical attribute: age threashold: 39.0\n",
      "preprocessing numerical attribute: balance threashold: 454.0\n",
      "preprocessing numerical attribute: day threashold: 16.0\n",
      "preprocessing numerical attribute: duration threashold: 179.0\n",
      "preprocessing numerical attribute: campaign threashold: 2.0\n",
      "preprocessing numerical attribute: pdays threashold: -1.0\n",
      "preprocessing numerical attribute: previous threashold: 0.0\n"
     ]
    }
   ],
   "source": [
    "attrib_name = {0:\"age\", 1:\"job\",2:\"marital\",3:\"education\",4:\"default\",5:\"balance\", 6:\"housing\",7:\"loan\",8:\"contact\",9:\"day\",10:\"month\",11:\"duration\",12:\"campaign\",13:\"pdays\",14:\"previous\",15:\"poutcome\"}\n",
    "label_values = [1, -1]\n",
    "\n",
    "categorical_attrib_values = { \"job\":     [\"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\", \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\"],\\\n",
    "                 \"marital\":  [\"married\",\"divorced\",\"single\"],\\\n",
    "                 \"education\":[\"unknown\",\"secondary\",\"primary\",\"tertiary\"],\\\n",
    "                 \"default\":  [\"yes\",\"no\"],\\\n",
    "                 \"housing\":  [\"yes\",\"no\"],\\\n",
    "                 \"loan\":     [\"yes\",\"no\"],\\\n",
    "                 \"contact\":  [\"unknown\",\"telephone\",\"cellular\"],\\\n",
    "                 \"month\":    [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"],\\\n",
    "                 \"poutcome\": [\"unknown\",\"other\",\"failure\",\"success\"]\n",
    "                }\n",
    "def convert_numerical_to_binary(df, Label_col_index):\n",
    "    for col_idx in range(Label_col_index):\n",
    "        if attrib_name[col_idx] not in categorical_attrib_values:\n",
    "            df[col_idx] = pd.to_numeric(df[col_idx], errors='coerce')\n",
    "            threashold = df[col_idx].median()\n",
    "            print(\"preprocessing numerical attribute:\", attrib_name[col_idx], \"threashold:\", threashold)\n",
    "            df[col_idx] = df[col_idx].apply(lambda x: 0 if int(x) <= threashold else 1)\n",
    "\n",
    "######## convert the numerical to binary\n",
    "train_df_processed = train_df\n",
    "test_df_processed = test_df\n",
    "convert_numerical_to_binary(train_df_processed, 16)\n",
    "convert_numerical_to_binary(test_df_processed, 16)\n",
    "\n",
    "######## convert label: yes->+1 no-> -1\n",
    "train_df_processed[16] = train_df_processed[16].apply(lambda x: -1 if x=='no' else 1)\n",
    "test_df_processed[16] = test_df_processed[16].apply(lambda x: -1 if x=='no' else 1)\n",
    "\n",
    "######## the the initial weights as a column to the dataframe\n",
    "Dtrain = [1/m_train] * m_train\n",
    "train_df_processed[17] = Dtrain\n",
    "# print(train_df_processed)\n",
    "Dtest  = [1/m_test]  * m_test\n",
    "test_df_processed[17] = Dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gain functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the information gain\n",
    "def entropy_gain(S,Label_col_index, attrib_idx): \n",
    "    H_S = S.groupby(Label_col_index)[Label_col_index + 1]\\\n",
    "    .apply(lambda x: (x.sum())*np.log2(x.sum()))\\\n",
    "    .sum()*-1\n",
    "    \n",
    "#     val0 = S.groupby([attrib_idx,Label_col_index])[Label_col_index].count()\n",
    "    val0 = S.groupby([attrib_idx,Label_col_index])[Label_col_index+1].sum()\n",
    "    val1 = S.groupby([attrib_idx])[Label_col_index+1].sum()\n",
    "#     print(val0)\n",
    "#     print(val1)\n",
    "    val2 = val0/val1\n",
    "    val3 = val2.apply(lambda x: x*math.log2(x)).reset_index(name='plog2p')\n",
    "    val4 = val3.groupby([attrib_idx])[\"plog2p\"].sum()*-1\n",
    "    val5 = S.groupby([attrib_idx])[attrib_idx].apply(lambda x: x.count()/S.shape[0])\n",
    "\n",
    "    Expected_H_Sv = (val4*val5).sum()\n",
    "\n",
    "#     print(\"H_S:\",H_S, \"Expected_H_Sv\",Expected_H_Sv, \"gain:\",H_S - Expected_H_Sv)\n",
    "    return H_S - Expected_H_Sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# id3 and prediction fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# returns the label that has the max weight\n",
    "# S: set of examples\n",
    "def select_label_with_max_weight_sum(S, Label_col_index):\n",
    "    return S.groupby([Label_col_index])[Label_col_index+1].sum().idxmax()\n",
    "    \n",
    "# returns the column index of the best splitter attribute\n",
    "# S: set of examples\n",
    "# Attributes: list of attributes to be evaluated\n",
    "# splitter_algorithm: the splitter algorithm, can be one of the 3 values (\"ME\":Majority Error, \"GI\":Gini Index, \"EN\":Entropy)\n",
    "def Best_spliter_attribute(S, Attributes, Label_col_index, splitter_algorithm):\n",
    "    if len(Attributes) < 2:\n",
    "        return Attributes[0]\n",
    "    best_gain = 0\n",
    "    best_attribute = Attributes[0] #\n",
    "    for v in Attributes:\n",
    "        if v != Label_col_index:\n",
    "            gain_v = 0\n",
    "            if splitter_algorithm == \"EN\":\n",
    "                gain_v = entropy_gain(S,Label_col_index, v)\n",
    "#                 print(\"attrib:\",attrib_name[v], \"gain:\",gain_v)\n",
    "            else:\n",
    "                assert False, \"Unknown splitter_algorithm:\" + splitter_algorithm + \"!!!\"\n",
    "            if gain_v > best_gain:\n",
    "                best_gain = gain_v\n",
    "                best_attribute = v\n",
    "#     print(\"best is:\",best_attribute, \"GAIN:\",best_gain)\n",
    "    return best_attribute\n",
    "\n",
    "def numeric_attrib_value(S, attrib_col_idx, numeric_value):\n",
    "    threashold = S[attrib_col_idx].median()\n",
    "    return numeric_value >= threashold   \n",
    "\n",
    "def predict(root, entry, Label_col_index):\n",
    "    example = {} \n",
    "    for i in range(Label_col_index):\n",
    "        example[attrib_name[i]] = entry[i]\n",
    "    return predict_helper(root, example)\n",
    "\n",
    "def predict_helper(root, example):\n",
    "    if isinstance(root, list): # if attrib-node\n",
    "        root_attrib_name = root[0]\n",
    "    else:\n",
    "        return root\n",
    "    example_attrib_val = example[root_attrib_name]\n",
    "    if isinstance(root[1][example_attrib_val], list): # if attrib-node\n",
    "        return predict_helper(root[1][example_attrib_val], example)\n",
    "    else: # if leaf node\n",
    "        return root[1][example_attrib_val]\n",
    "    \n",
    "def predict_dataset(S, root, Label_col_index):\n",
    "    all = 0\n",
    "    correct = 0\n",
    "    for idx, row in S.iterrows():\n",
    "        all += 1\n",
    "        gold_label = row[Label_col_index]\n",
    "        predicted_label = predict(root, row, Label_col_index)\n",
    "        if predicted_label == gold_label:\n",
    "            correct +=1\n",
    "    return correct / all # accuracy \n",
    "        \n",
    "# ##############              ID3 implementation:\n",
    "# Input:\n",
    "# S: the set of Examples\n",
    "# Attributes: the set of measured attributes\n",
    "# Label_col_index: column index of the target attribute (the prediction)\n",
    "# max_tree_level: bounds the height of the tree\n",
    "# splitter_algorithm: can be one of the 3 values (\"ME\":Majority Error, \"GI\":Gini Index, \"EN\":Entropy)\n",
    "def ID3(S, Attributes, Label_col_index, max_tree_level, splitter_algorithm):\n",
    "    if(max_tree_level == 0):                                                            # if at max level\n",
    "#         print(\"max_tree_level reached\")\n",
    "        return select_label_with_max_weight_sum(S, Label_col_index)\n",
    "    elif S[Label_col_index].nunique() == 1:                                             # if all examples have same label:   \n",
    "#         print(\"Label_col_index unique\")\n",
    "        return select_label_with_max_weight_sum(S, Label_col_index)\n",
    "    elif len(Attributes) == 0:                                                          # if Attributes empty\n",
    "#         print(\"Attributes\")\n",
    "        return select_label_with_max_weight_sum(S, Label_col_index)\n",
    "    else:\n",
    "        # 1. Create a Root node for tree\n",
    "        Root = [] # each \"attribute node\" is a list s.t. \n",
    "                                                    # 1st element = string attribute name\n",
    "                                                    # 2nd element = dictionary children;\n",
    "                                                            # key = each possible attribute value v\n",
    "                                                            # value = an \"attribute node\" list;  or a string label for leaf nodes\n",
    "        # 2. A = attribute in Attributes that best splits S\n",
    "        A = Best_spliter_attribute(S, Attributes, Label_col_index, splitter_algorithm)\n",
    "#         print(\"best is:\",attrib_name[A])\n",
    "        Root.append(attrib_name[A]) # 1st element = string attribute name\n",
    "        Root.append({})             # 2nd element = dictionary children;\n",
    "        # 3. for each possible value v of that A can take:\n",
    "        attribute_values=[]\n",
    "        if(attrib_name[A] in categorical_attrib_values):\n",
    "            attribute_values = categorical_attrib_values[attrib_name[A]]\n",
    "        else: # o.w. it is numerical \n",
    "            attribute_values = [0,1]\n",
    "        for v in attribute_values:\n",
    "            # 1. Add a new tree branch corresponding to A=v\n",
    "            # 2. Let Sv be the subset of examples in S with A=v\n",
    "            Sv = S.loc[S[A] == v]\n",
    "            if len(Sv) == 0: # if Sv is empty\n",
    "#                 print(\"Sv is empty\")\n",
    "                Root[1][v] = select_label_with_max_weight_sum(S, Label_col_index) # string label\n",
    "            else:\n",
    "                Attrib_minus_A = Attributes\n",
    "                if len(Attrib_minus_A) > 0 and A in Attrib_minus_A:\n",
    "                    Attrib_minus_A.remove(A)\n",
    "                Root[1][v] = ID3(Sv, Attrib_minus_A,Label_col_index, max_tree_level-1,splitter_algorithm) # an \"attribute node\" list;\n",
    "        return Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some  Training examples ...\n",
      "['duration', {0: -1, 1: -1}]\n",
      "#######\n",
      "train accuracy: 0.8808\n",
      "test accuracy: 0.8752\n"
     ]
    }
   ],
   "source": [
    "# ##############              test the decision stump\n",
    "print(\"Some  Training examples ...\")\n",
    "Attributes = [0,1,2,3,4,5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] # initially put all attributes except the label in Attributes set\n",
    "tree_infoGain = ID3(train_df_processed, Attributes,16,1, \"EN\")  # note the max_height is set to 2 to construct decision stumps\n",
    "print(tree_infoGain)\n",
    "\n",
    "print(\"#######\")\n",
    "print(\"train accuracy:\",predict_dataset(train_df_processed, tree_infoGain,16))\n",
    "print(\"test accuracy:\",predict_dataset(test_df_processed, tree_infoGain,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_processed.to_csv(\"sample.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Et(S, root, Label_col_index):\n",
    "    Et = 0\n",
    "    ht_x = []\n",
    "    for idx, row in S.iterrows():\n",
    "        gold_label = row[Label_col_index]\n",
    "        predicted_label = predict(root, row, Label_col_index)\n",
    "        ht_x.append(predicted_label)\n",
    "        if predicted_label != gold_label:\n",
    "            Et += row[Label_col_index + 1]\n",
    "    return Et, ht_x\n",
    "\n",
    "# def get_Et(S, root, Label_col_index):\n",
    "#     ht_x = []\n",
    "#     for idx, row in S.iterrows():\n",
    "#         predicted_label = predict(root, row, Label_col_index)\n",
    "#         ht_x.append(predicted_label)\n",
    "#     Et = 0.5 - 0.5 * (sum(S[Label_col_index+1].mul(S[Label_col_index]).mul(pd.Series(ht_x))))\n",
    "#     return Et, ht_x\n",
    "\n",
    "# ##############              AdaBoost_train implementation:\n",
    "# Input:\n",
    "# S: the set of Examples\n",
    "# Attributes: the set of measured attributes\n",
    "# Label_col_index: column index of the target attribute (the prediction)\n",
    "# T: number of iterations\n",
    "def AdaBoost_train(S, Label_col_index, T):\n",
    "    print(\"AdaBoost Training...\")\n",
    "    print(\"Total iterations:\",T)\n",
    "    \n",
    "    # 1. Initialize D1(i) = 1/m for all i = 1, 2, ..., m\n",
    "    h=[]\n",
    "    alpha=[]\n",
    "    # 2. For t = 1, 2, ..., T:\n",
    "    for t in range(T):\n",
    "        print(\"iteration:\",t+1)\n",
    "        Attributes = [0,1,2,3,4,5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "        # 1. Find a classifier ht whose weighted classification error is better than chance\n",
    "#         print(S[Label_col_index+1])\n",
    "        Hyp = ID3(S, Attributes, Label_col_index, 1, \"EN\")\n",
    "#         print(Hyp)\n",
    "        print(\"adaboost stump \",t+1 ,\" train_accuracy:\",predict_dataset(train_df_processed, Hyp,16),\" test_accuracy:\",predict_dataset(test_df_processed, Hyp,16))\n",
    "        h.append(Hyp)\n",
    "        # compute the Et\n",
    "        Et = 0\n",
    "        ht_x = []\n",
    "        Et, ht_x = get_Et(S,Hyp, Label_col_index)\n",
    "        # compute Zt        \n",
    "#         print(\"Dt(i):\")\n",
    "#         print(S[Label_col_index+1])\n",
    "#         print(\"Et:\",Et)\n",
    "        # 2. Compute its vote\n",
    "        Alp = 0.5 * math.log((1-Et)/Et)\n",
    "#         print(\"Alp:\",Alp)\n",
    "        alpha.append(Alp)\n",
    "        # 3. Update the  values of the weights for the training examples\n",
    "        temp = S[Label_col_index].mul(pd.Series(ht_x))\n",
    "#         print(\"yi*htx:\")\n",
    "#         print(temp)\n",
    "        \n",
    "        temp = temp.apply(lambda x: math.exp(-1 * Alp *x))\n",
    "#         print(\"exp(-1 * Alp * yi*htx)\")\n",
    "#         print(temp)\n",
    "        S[Label_col_index+1] = S[Label_col_index+1].mul(temp)\n",
    "#         print(\"Dt(i)*exp(-1 * Alp * yi*htx)\")\n",
    "#         print(S[Label_col_index+1])\n",
    "\n",
    "#         print(\"sum\")\n",
    "#         print(sum(S[Label_col_index+1]))\n",
    "#         S[Label_col_index+1]/= (2*math.sqrt(Et*(1-Et)))\n",
    "        S[Label_col_index+1]/=sum(S[Label_col_index+1])\n",
    "#         print(\"Dt(i)/Zt*exp(-1 * Alp * yi*htx)\")\n",
    "#         print(S[Label_col_index+1])\n",
    "\n",
    "#         print(sum(S[Label_col_index+1]))\n",
    "#         print(Et,Alp)\n",
    "    # 3. Return the final hypothesis\n",
    "    return h, alpha\n",
    "\n",
    "# ##############              AdaBoost_predict implementation:\n",
    "# Input: \n",
    "# h[1:T]: list of decision stumps\n",
    "# alpha[1:T]: list of votes\n",
    "# x: example to be predicted\n",
    "def AdaBoost_predict(h, alpha, x, Label_col_index):\n",
    "    ht_x =[]\n",
    "    for t in range(len(h)):\n",
    "        ht_x.append(predict(h[t],x,Label_col_index))\n",
    "    return np.sign(np.dot(ht_x, alpha))\n",
    "\n",
    "def AdaBoost_predict_dataset(S, h,alpha, Label_col_index):\n",
    "    all = 0\n",
    "    correct = 0\n",
    "    for idx, row in S.iterrows():\n",
    "        all += 1\n",
    "        gold_label = row[Label_col_index]\n",
    "        predicted_label = AdaBoost_predict(h,alpha, row, Label_col_index)\n",
    "        if predicted_label == gold_label:\n",
    "            correct +=1\n",
    "    return correct / all # accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#############################\n",
      "AdaBoost Training...\n",
      "Total iterations: 20\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "iteration: 8\n",
      "iteration: 9\n",
      "iteration: 10\n",
      "iteration: 11\n",
      "iteration: 12\n",
      "iteration: 13\n",
      "iteration: 14\n",
      "iteration: 15\n",
      "iteration: 16\n",
      "iteration: 17\n",
      "iteration: 18\n",
      "iteration: 19\n",
      "iteration: 20\n",
      "#######\n",
      "train accuracy: 0.8954\n",
      "test accuracy: 0.892\n",
      "t\troot node:\talpha\n",
      "================================\n",
      "0 \t duration \t 1.0000139153886038\n",
      "1 \t duration \t 0.44971927433442077\n",
      "2 \t month \t 0.3382556946117501\n",
      "3 \t contact \t 0.16150414464196464\n",
      "4 \t poutcome \t 0.22564141686046688\n",
      "5 \t month \t 0.17103460917669522\n",
      "6 \t housing \t 0.1763605990484829\n",
      "7 \t duration \t 0.0974438170387874\n",
      "8 \t job \t 0.11420555679497812\n",
      "9 \t loan \t 0.05362171501918158\n",
      "10 \t poutcome \t 0.14761382721056945\n",
      "11 \t marital \t 0.11272659040279949\n",
      "12 \t month \t 0.09193582629697968\n",
      "13 \t job \t 0.05828785861580768\n",
      "14 \t loan \t 0.05723109442784649\n",
      "15 \t month \t 0.09617756952203509\n",
      "16 \t campaign \t 0.07617324020516408\n",
      "17 \t education \t 0.0838659308818826\n",
      "18 \t job \t 0.05939718472502053\n",
      "19 \t poutcome \t 0.05875097017092982\n"
     ]
    }
   ],
   "source": [
    "######## the the initial weights as a column to the dataframe\n",
    "Dtrain = [1/m_train] * m_train\n",
    "train_df_processed[17] = Dtrain\n",
    "\n",
    "Dtest  = [1/m_test]  * m_test\n",
    "test_df_processed[17] = Dtest\n",
    "# ##############              test the AdaBoost\n",
    "print(\"\\n\\n#############################\")\n",
    "h, alpha = AdaBoost_train(train_df_processed, 16, 20)\n",
    "print(\"#######\")\n",
    "print(\"train accuracy:\",AdaBoost_predict_dataset(train_df_processed,h,alpha,16))\n",
    "print(\"test accuracy:\",AdaBoost_predict_dataset(test_df_processed,h,alpha,16))\n",
    "\n",
    "print(\"t\\troot node:\\talpha\")\n",
    "print(\"================================\")\n",
    "for i in  range(len(h)):\n",
    "    print(i, \"\\t\", h[i][0], \"\\t\", alpha[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
