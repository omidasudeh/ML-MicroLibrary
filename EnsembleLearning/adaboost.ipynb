{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "test_df = pd.read_csv(\"./bank/test.csv\", header=None)\n",
    "train_df = pd.read_csv(\"./bank/train.csv\", header=None)\n",
    "m_train = len(train_df)\n",
    "m_test = len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing numerical attribute: age threashold: 38.0\n",
      "preprocessing numerical attribute: balance threashold: 452.5\n",
      "preprocessing numerical attribute: day threashold: 16.0\n",
      "preprocessing numerical attribute: duration threashold: 180.0\n",
      "preprocessing numerical attribute: campaign threashold: 2.0\n",
      "preprocessing numerical attribute: pdays threashold: -1.0\n",
      "preprocessing numerical attribute: previous threashold: 0.0\n",
      "preprocessing numerical attribute: age threashold: 39.0\n",
      "preprocessing numerical attribute: balance threashold: 454.0\n",
      "preprocessing numerical attribute: day threashold: 16.0\n",
      "preprocessing numerical attribute: duration threashold: 179.0\n",
      "preprocessing numerical attribute: campaign threashold: 2.0\n",
      "preprocessing numerical attribute: pdays threashold: -1.0\n",
      "preprocessing numerical attribute: previous threashold: 0.0\n"
     ]
    }
   ],
   "source": [
    "attrib_name = {0:\"age\", 1:\"job\",2:\"marital\",3:\"education\",4:\"default\",5:\"balance\", 6:\"housing\",7:\"loan\",8:\"contact\",9:\"day\",10:\"month\",11:\"duration\",12:\"campaign\",13:\"pdays\",14:\"previous\",15:\"poutcome\"}\n",
    "label_values = [\"yes\", \"no\"]\n",
    "\n",
    "categorical_attrib_values = { \"job\":     [\"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\", \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\"],\\\n",
    "                 \"marital\":  [\"married\",\"divorced\",\"single\"],\\\n",
    "                 \"education\":[\"unknown\",\"secondary\",\"primary\",\"tertiary\"],\\\n",
    "                 \"default\":  [\"yes\",\"no\"],\\\n",
    "                 \"housing\":  [\"yes\",\"no\"],\\\n",
    "                 \"loan\":     [\"yes\",\"no\"],\\\n",
    "                 \"contact\":  [\"unknown\",\"telephone\",\"cellular\"],\\\n",
    "                 \"month\":    [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"],\\\n",
    "                 \"poutcome\": [\"unknown\",\"other\",\"failure\",\"success\"]\n",
    "                }\n",
    "def convert_numerical_to_binary(df, Label_col_index):\n",
    "    for col_idx in range(Label_col_index):\n",
    "        if attrib_name[col_idx] not in categorical_attrib_values:\n",
    "            df[col_idx] = pd.to_numeric(df[col_idx], errors='coerce')\n",
    "            threashold = df[col_idx].median()\n",
    "            print(\"preprocessing numerical attribute:\", attrib_name[col_idx], \"threashold:\", threashold)\n",
    "            df[col_idx] = df[col_idx].apply(lambda x: 0 if int(x) <= threashold else 1)\n",
    "\n",
    "######## convert the numerical to binary\n",
    "train_df_processed = train_df\n",
    "test_df_processed = test_df\n",
    "convert_numerical_to_binary(train_df_processed, 16)\n",
    "convert_numerical_to_binary(test_df_processed, 16)\n",
    "\n",
    "######## convert label: yes->+1 no-> -1\n",
    "train_df_processed[16] = train_df_processed[16].apply(lambda x: -1 if x=='no' else 1)\n",
    "test_df_processed[16] = test_df_processed[16].apply(lambda x: -1 if x=='no' else 1)\n",
    "\n",
    "######## the the initial weights as a column to the dataframe\n",
    "Dtrain = [1/m_train] * m_train\n",
    "train_df_processed[17] = Dtrain\n",
    "\n",
    "Dtest  = [1/m_test]  * m_test\n",
    "test_df_processed[17] = Dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gain functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the information gain\n",
    "def entropy_gain(S,Label_col_index, attrib_idx):\n",
    "    H_S = S.groupby(Label_col_index)[Label_col_index + 1]\\\n",
    "    .apply(lambda x: (x.sum()/S.shape[0])*np.log2(x.sum()/S.shape[0]))\\\n",
    "    .sum()*-1\n",
    "    \n",
    "    Expected_H_Sv = S.groupby([attrib_idx,Label_col_index],as_index=False)[Label_col_index+1].sum()\\\n",
    "    .groupby(attrib_idx)[Label_col_index+1].apply(lambda x:(x.sum()/S.shape[0])*((x/x.sum())*np.log2(x/x.sum()))).sum()*-1\n",
    "        \n",
    "    return H_S - Expected_H_Sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# id3 and prediction fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# returns the column index of the best splitter attribute\n",
    "# S: set of examples\n",
    "# Attributes: list of attributes to be evaluated\n",
    "# splitter_algorithm: the splitter algorithm, can be one of the 3 values (\"ME\":Majority Error, \"GI\":Gini Index, \"EN\":Entropy)\n",
    "def Best_spliter_attribute(S, Attributes, Label_col_index, splitter_algorithm):\n",
    "    if len(Attributes) < 2:\n",
    "        return Attributes[0]\n",
    "    best_gain = 0\n",
    "    best_attribute = Attributes[0]\n",
    "    for v in Attributes:\n",
    "        if v != Label_col_index:\n",
    "            gain_v = 0\n",
    "            if splitter_algorithm == \"EN\":\n",
    "                gain_v = entropy_gain(S,Label_col_index, v)\n",
    "            elif splitter_algorithm == \"ME\":\n",
    "                gain_v = ME_gain(S,Label_col_index,v)\n",
    "            elif splitter_algorithm == \"GI\":\n",
    "                gain_v = gini_gain(S,Label_col_index,v)\n",
    "            else:\n",
    "                assert False, \"Unknown splitter_algorithm:\" + splitter_algorithm + \"!!!\"\n",
    "            if gain_v > best_gain:\n",
    "                best_gain = gain_v\n",
    "                best_attribute = v\n",
    "#     print(\"best attrib is:\",best_attribute)\n",
    "    return best_attribute\n",
    "\n",
    "def numeric_attrib_value(S, attrib_col_idx, numeric_value):\n",
    "    threashold = S[attrib_col_idx].median()\n",
    "    return numeric_value >= threashold   \n",
    "\n",
    "def predict(root, entry, Label_col_index):\n",
    "    example = {} \n",
    "    for i in range(Label_col_index):\n",
    "        example[attrib_name[i]] = entry[i]\n",
    "    return predict_helper(root, example)\n",
    "\n",
    "def predict_helper(root, example):\n",
    "    if isinstance(root, list): # if attrib-node\n",
    "        root_attrib_name = root[0]\n",
    "    else:\n",
    "        return root\n",
    "    example_attrib_val = example[root_attrib_name]\n",
    "    if isinstance(root[1][example_attrib_val], list): # if attrib-node\n",
    "        return predict_helper(root[1][example_attrib_val], example)\n",
    "    else: # if leaf node\n",
    "        return root[1][example_attrib_val]\n",
    "    \n",
    "def predict_dataset(S, root, Label_col_index):\n",
    "    all = 0\n",
    "    correct = 0\n",
    "    for idx, row in S.iterrows():\n",
    "        all += 1\n",
    "        gold_label = row[Label_col_index]\n",
    "        predicted_label = predict(root, row, Label_col_index)\n",
    "        if predicted_label == gold_label:\n",
    "            correct +=1\n",
    "    return correct / all # accuracy \n",
    "\n",
    "def get_Et(S, root, Label_col_index):\n",
    "    Et = 0\n",
    "    ht_x = []\n",
    "    for idx, row in S.iterrows():\n",
    "        gold_label = row[Label_col_index]\n",
    "        predicted_label = predict(root, row, Label_col_index)\n",
    "        ht_x.append(predicted_label)\n",
    "        if predicted_label != gold_label:\n",
    "            Et += row[Label_col_index + 1]\n",
    "    return Et, ht_x\n",
    "        \n",
    "# ##############              ID3 implementation:\n",
    "# Input:\n",
    "# S: the set of Examples\n",
    "# Attributes: the set of measured attributes\n",
    "# Label_col_index: column index of the target attribute (the prediction)\n",
    "# max_tree_level: bounds the height of the tree\n",
    "# splitter_algorithm: can be one of the 3 values (\"ME\":Majority Error, \"GI\":Gini Index, \"EN\":Entropy)\n",
    "def ID3(S, Attributes, Label_col_index, max_tree_level, splitter_algorithm):\n",
    "    if(max_tree_level == 0):                                                            # if at max level\n",
    "        return S[Label_col_index].mode()[0]   \n",
    "    elif S[Label_col_index].nunique() == 1:                                             # if all examples have same label:   \n",
    "        return S[Label_col_index].mode()[0]\n",
    "    elif len(Attributes) == 0:                                                          # if Attributes empty\n",
    "        return S[Label_col_index].mode()[0]\n",
    "    else:\n",
    "        # 1. Create a Root node for tree\n",
    "        Root = [] # each \"attribute node\" is a list s.t. \n",
    "                                                    # 1st element = string attribute name\n",
    "                                                    # 2nd element = dictionary children;\n",
    "                                                            # key = each possible attribute value v\n",
    "                                                            # value = an \"attribute node\" list;  or a string label for leaf nodes\n",
    "        # 2. A = attribute in Attributes that best splits S\n",
    "        A = Best_spliter_attribute(S, Attributes, Label_col_index, splitter_algorithm)\n",
    "        Root.append(attrib_name[A]) # 1st element = string attribute name\n",
    "        Root.append({})             # 2nd element = dictionary children;\n",
    "        # 3. for each possible value v of that A can take:\n",
    "        attribute_values=[]\n",
    "        if(attrib_name[A] in categorical_attrib_values):\n",
    "            attribute_values = categorical_attrib_values[attrib_name[A]]\n",
    "        else: # o.w. it is numerical \n",
    "            attribute_values = [0,1]\n",
    "        for v in attribute_values:\n",
    "            # 1. Add a new tree branch corresponding to A=v\n",
    "            # 2. Let Sv be the subset of examples in S with A=v\n",
    "            Sv = S.loc[S[A] == v]\n",
    "            if len(Sv) == 0: # if Sv is empty\n",
    "                Root[1][v] = S[Label_col_index].mode()[0] # string label\n",
    "            else:\n",
    "                Attrib_minus_A = Attributes\n",
    "                if len(Attrib_minus_A) > 0 and A in Attrib_minus_A:\n",
    "                    Attrib_minus_A.remove(A)\n",
    "                Root[1][v] = ID3(Sv, Attrib_minus_A,Label_col_index, max_tree_level-1,splitter_algorithm) # an \"attribute node\" list;\n",
    "        return Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some  Training examples ...\n",
      "['duration', {0: -1, 1: -1}]\n",
      "#######\n",
      "train accuracy: 0.8808\n",
      "test accuracy: 0.8752\n"
     ]
    }
   ],
   "source": [
    "# ##############              test the decision stump\n",
    "print(\"Some  Training examples ...\")\n",
    "Attributes = [0,1,2,3,4,5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] # initially put all attributes except the label in Attributes set\n",
    "tree_infoGain = ID3(train_df_processed, Attributes,16,1, \"EN\")  # note the max_height is set to 2 to construct decision stumps\n",
    "print(tree_infoGain)\n",
    "\n",
    "print(\"#######\")\n",
    "print(\"train accuracy:\",predict_dataset(train_df_processed, tree_infoGain,16))\n",
    "print(\"test accuracy:\",predict_dataset(test_df_processed, tree_infoGain,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############              AdaBoost_train implementation:\n",
    "# Input:\n",
    "# S: the set of Examples\n",
    "# Attributes: the set of measured attributes\n",
    "# Label_col_index: column index of the target attribute (the prediction)\n",
    "# T: number of iterations\n",
    "def AdaBoost_train(S, Attributes, Label_col_index, T):\n",
    "    print(\"AdaBoost Training...\")\n",
    "    print(\"Total iterations:\",T)\n",
    "    \n",
    "    # 1. Initialize D1(i) = 1/m for all i = 1, 2, ..., m\n",
    "    h=[]\n",
    "    alpha=[]\n",
    "    # 2. For t = 1, 2, ..., T:\n",
    "    for t in range(T):\n",
    "        print(\"iteration:\",t)\n",
    "        # 1. Find a classifier ht whose weighted classification error is better than chance\n",
    "        Hyp = ID3(S, Attributes, Label_col_index, 1, \"EN\")\n",
    "#         print(\"stump_\",t ,\" test accuracy:\",predict_dataset(test_df_processed, Hyp,16))\n",
    "        h.append(Hyp)\n",
    "        # compute the Et\n",
    "        Et, ht_x = get_Et(S,Hyp, Label_col_index)\n",
    "        # compute Zt\n",
    "        Zt = S[Label_col_index+1].sum()\n",
    "        \n",
    "#         Zt = 2*math.sqrt(Et * (1 - Et))\n",
    "        # 2. Compute its vote\n",
    "        Alp = 0.5 * math.log((1-Et)/Et)\n",
    "        alpha.append(Alp)\n",
    "        # 3. Update the  values of the weights for the training examples\n",
    "        temp = S[Label_col_index+1].mul(pd.Series(ht_x))\n",
    "        temp = temp.apply(lambda x: math.exp(-1*Alp*x))\n",
    "        S[Label_col_index+1] = (S[Label_col_index+1]/Zt).mul(temp)\n",
    "    # 3. Return the final hypothesis\n",
    "    return h, alpha\n",
    "\n",
    "# ##############              AdaBoost_predict implementation:\n",
    "# Input: \n",
    "# h[1:T]: list of decision stumps\n",
    "# alpha[1:T]: list of votes\n",
    "# x: example to be predicted\n",
    "def AdaBoost_predict(h, alpha, x, Label_col_index):\n",
    "    ht_x =[]\n",
    "    for t in range(len(h)):\n",
    "        ht_x.append(predict(h[t],x,Label_col_index))\n",
    "    return np.sign(np.dot(ht_x, alpha))\n",
    "\n",
    "def AdaBoost_predict_dataset(S, h,alpha, Label_col_index):\n",
    "    all = 0\n",
    "    correct = 0\n",
    "    for idx, row in S.iterrows():\n",
    "        all += 1\n",
    "        gold_label = row[Label_col_index]\n",
    "        predicted_label = AdaBoost_predict(h,alpha, row, Label_col_index)\n",
    "        if predicted_label == gold_label:\n",
    "            correct +=1\n",
    "    return correct / all # accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Training...\n",
      "Total iterations: 100\n",
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "iteration: 8\n",
      "iteration: 9\n",
      "iteration: 10\n",
      "iteration: 11\n",
      "iteration: 12\n",
      "iteration: 13\n",
      "iteration: 14\n",
      "iteration: 15\n",
      "iteration: 16\n",
      "iteration: 17\n",
      "iteration: 18\n",
      "iteration: 19\n",
      "iteration: 20\n",
      "iteration: 21\n",
      "iteration: 22\n",
      "iteration: 23\n",
      "iteration: 24\n",
      "iteration: 25\n",
      "iteration: 26\n",
      "iteration: 27\n",
      "iteration: 28\n",
      "iteration: 29\n",
      "iteration: 30\n",
      "iteration: 31\n",
      "iteration: 32\n",
      "iteration: 33\n",
      "iteration: 34\n",
      "iteration: 35\n",
      "iteration: 36\n",
      "iteration: 37\n",
      "iteration: 38\n",
      "iteration: 39\n",
      "iteration: 40\n",
      "iteration: 41\n",
      "iteration: 42\n",
      "iteration: 43\n",
      "iteration: 44\n",
      "iteration: 45\n",
      "iteration: 46\n",
      "iteration: 47\n",
      "iteration: 48\n",
      "iteration: 49\n",
      "iteration: 50\n",
      "iteration: 51\n",
      "iteration: 52\n",
      "iteration: 53\n",
      "iteration: 54\n",
      "iteration: 55\n",
      "iteration: 56\n",
      "iteration: 57\n",
      "iteration: 58\n",
      "iteration: 59\n",
      "iteration: 60\n",
      "iteration: 61\n",
      "iteration: 62\n",
      "iteration: 63\n",
      "iteration: 64\n",
      "iteration: 65\n",
      "iteration: 66\n",
      "iteration: 67\n",
      "iteration: 68\n",
      "iteration: 69\n",
      "iteration: 70\n",
      "iteration: 71\n",
      "iteration: 72\n",
      "iteration: 73\n",
      "iteration: 74\n",
      "iteration: 75\n",
      "iteration: 76\n",
      "iteration: 77\n",
      "iteration: 78\n",
      "iteration: 79\n",
      "iteration: 80\n",
      "iteration: 81\n",
      "iteration: 82\n",
      "iteration: 83\n",
      "iteration: 84\n",
      "iteration: 85\n",
      "iteration: 86\n",
      "iteration: 87\n",
      "iteration: 88\n",
      "iteration: 89\n",
      "iteration: 90\n",
      "iteration: 91\n",
      "iteration: 92\n",
      "iteration: 93\n",
      "iteration: 94\n",
      "iteration: 95\n",
      "iteration: 96\n",
      "iteration: 97\n",
      "iteration: 98\n",
      "iteration: 99\n",
      "#######\n",
      "train accuracy: 0.8808\n",
      "test accuracy: 0.8752\n"
     ]
    }
   ],
   "source": [
    "# ##############              test the AdaBoost\n",
    "Attributes = [0,1,2,3,4,5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] # initially put all attributes except the label in Attributes set\n",
    "h, alpha = AdaBoost_train(train_df_processed, Attributes, 16, 100)\n",
    "print(\"#######\")\n",
    "print(\"train accuracy:\",AdaBoost_predict_dataset(train_df_processed,h,alpha,16))\n",
    "print(\"test accuracy:\",AdaBoost_predict_dataset(test_df_processed,h,alpha,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
