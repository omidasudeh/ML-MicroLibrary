{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy as cp\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "test_df = pd.read_csv(\"../data/bank/test.csv\", header=None)\n",
    "m_test = len(test_df)\n",
    "\n",
    "train_df = pd.read_csv(\"../data/bank/train.csv\", header=None)\n",
    "m_train = len(train_df)\n",
    "\n",
    "# train_df = pd.read_csv(\"./bank/sample1.csv\", header=None)\n",
    "# m_train = len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing numerical attribute: age threashold: 38.0\n",
      "preprocessing numerical attribute: balance threashold: 452.5\n",
      "preprocessing numerical attribute: day threashold: 16.0\n",
      "preprocessing numerical attribute: duration threashold: 180.0\n",
      "preprocessing numerical attribute: campaign threashold: 2.0\n",
      "preprocessing numerical attribute: pdays threashold: -1.0\n",
      "preprocessing numerical attribute: previous threashold: 0.0\n",
      "preprocessing numerical attribute: age threashold: 39.0\n",
      "preprocessing numerical attribute: balance threashold: 454.0\n",
      "preprocessing numerical attribute: day threashold: 16.0\n",
      "preprocessing numerical attribute: duration threashold: 179.0\n",
      "preprocessing numerical attribute: campaign threashold: 2.0\n",
      "preprocessing numerical attribute: pdays threashold: -1.0\n",
      "preprocessing numerical attribute: previous threashold: 0.0\n"
     ]
    }
   ],
   "source": [
    "attrib_name = {0:\"age\", 1:\"job\",2:\"marital\",3:\"education\",4:\"default\",5:\"balance\", 6:\"housing\",7:\"loan\",8:\"contact\",9:\"day\",10:\"month\",11:\"duration\",12:\"campaign\",13:\"pdays\",14:\"previous\",15:\"poutcome\"}\n",
    "label_values = [1, -1]\n",
    "\n",
    "categorical_attrib_values = { \"job\":     [\"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\", \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\"],\\\n",
    "                 \"marital\":  [\"married\",\"divorced\",\"single\"],\\\n",
    "                 \"education\":[\"unknown\",\"secondary\",\"primary\",\"tertiary\"],\\\n",
    "                 \"default\":  [\"yes\",\"no\"],\\\n",
    "                 \"housing\":  [\"yes\",\"no\"],\\\n",
    "                 \"loan\":     [\"yes\",\"no\"],\\\n",
    "                 \"contact\":  [\"unknown\",\"telephone\",\"cellular\"],\\\n",
    "                 \"month\":    [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"],\\\n",
    "                 \"poutcome\": [\"unknown\",\"other\",\"failure\",\"success\"]\n",
    "                }\n",
    "def convert_numerical_to_binary(df, Label_col_index):\n",
    "    for col_idx in range(Label_col_index):\n",
    "        if attrib_name[col_idx] not in categorical_attrib_values:\n",
    "            df[col_idx] = pd.to_numeric(df[col_idx], errors='coerce')\n",
    "            threashold = df[col_idx].median()\n",
    "            print(\"preprocessing numerical attribute:\", attrib_name[col_idx], \"threashold:\", threashold)\n",
    "            df[col_idx] = df[col_idx].apply(lambda x: 0 if int(x) <= threashold else 1)\n",
    "\n",
    "######## convert the numerical to binary\n",
    "train_df_processed = train_df\n",
    "test_df_processed = test_df\n",
    "convert_numerical_to_binary(train_df_processed, 16)\n",
    "convert_numerical_to_binary(test_df_processed, 16)\n",
    "\n",
    "######## convert label: yes->+1 no-> -1\n",
    "train_df_processed[16] = train_df_processed[16].apply(lambda x: -1 if x=='no' else 1)\n",
    "test_df_processed[16] = test_df_processed[16].apply(lambda x: -1 if x=='no' else 1)\n",
    "\n",
    "######## the the initial weights as a column to the dataframe\n",
    "Dtrain = [1/m_train] * m_train\n",
    "train_df_processed[17] = Dtrain\n",
    "# print(train_df_processed)\n",
    "Dtest  = [1/m_test]  * m_test\n",
    "test_df_processed[17] = Dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gain functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the information gain\n",
    "def entropy_gain(S,Label_col_index, attrib_idx): \n",
    "    H_S = S.groupby(Label_col_index)[Label_col_index + 1]\\\n",
    "    .apply(lambda x: (x.sum())*np.log2(x.sum()))\\\n",
    "    .sum()*-1\n",
    "    \n",
    "#     val0 = S.groupby([attrib_idx,Label_col_index])[Label_col_index].count()\n",
    "    val0 = S.groupby([attrib_idx,Label_col_index])[Label_col_index+1].sum()\n",
    "    val1 = S.groupby([attrib_idx])[Label_col_index+1].sum()\n",
    "#     print(val0)\n",
    "#     print(val1)\n",
    "    val2 = val0/val1\n",
    "    val3 = val2.apply(lambda x: x*math.log2(x)).reset_index(name='plog2p')\n",
    "    val4 = val3.groupby([attrib_idx])[\"plog2p\"].sum()*-1\n",
    "    val5 = S.groupby([attrib_idx])[attrib_idx].apply(lambda x: x.count()/S.shape[0])\n",
    "\n",
    "    Expected_H_Sv = (val4*val5).sum()\n",
    "\n",
    "#     print(\"H_S:\",H_S, \"Expected_H_Sv\",Expected_H_Sv, \"gain:\",H_S - Expected_H_Sv)\n",
    "    return H_S - Expected_H_Sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# id3 and prediction fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# returns the label that has the max weight\n",
    "# S: set of examples\n",
    "def select_label_with_max_weight_sum(S, Label_col_index):\n",
    "    return S.groupby([Label_col_index])[Label_col_index+1].sum().idxmax()\n",
    "    \n",
    "# returns the column index of the best splitter attribute\n",
    "# S: set of examples\n",
    "# Attributes: list of attributes to be evaluated\n",
    "# splitter_algorithm: the splitter algorithm, can be one of the 3 values (\"ME\":Majority Error, \"GI\":Gini Index, \"EN\":Entropy)\n",
    "def Best_spliter_attribute(S, Attributes, Label_col_index, splitter_algorithm):\n",
    "    if len(Attributes) < 2:\n",
    "        return Attributes[0]\n",
    "    best_gain = 0\n",
    "    best_attribute = Attributes[0] #\n",
    "    for v in Attributes:\n",
    "        if v != Label_col_index:\n",
    "            gain_v = 0\n",
    "            if splitter_algorithm == \"EN\":\n",
    "                gain_v = entropy_gain(S,Label_col_index, v)\n",
    "#                 print(\"attrib:\",attrib_name[v], \"gain:\",gain_v)\n",
    "            else:\n",
    "                assert False, \"Unknown splitter_algorithm:\" + splitter_algorithm + \"!!!\"\n",
    "            if gain_v > best_gain:\n",
    "                best_gain = gain_v\n",
    "                best_attribute = v\n",
    "#     print(\"best is:\",best_attribute, \"GAIN:\",best_gain)\n",
    "    return best_attribute\n",
    "\n",
    "def numeric_attrib_value(S, attrib_col_idx, numeric_value):\n",
    "    threashold = S[attrib_col_idx].median()\n",
    "    return numeric_value >= threashold   \n",
    "\n",
    "def predict(root, entry, Label_col_index):\n",
    "    example = {} \n",
    "    for i in range(Label_col_index):\n",
    "        example[attrib_name[i]] = entry[i]\n",
    "    return predict_helper(root, example)\n",
    "\n",
    "def predict_helper(root, example):\n",
    "    if isinstance(root, list): # if attrib-node\n",
    "        root_attrib_name = root[0]\n",
    "    else:\n",
    "        return root\n",
    "    example_attrib_val = example[root_attrib_name]\n",
    "    if isinstance(root[1][example_attrib_val], list): # if attrib-node\n",
    "        return predict_helper(root[1][example_attrib_val], example)\n",
    "    else: # if leaf node\n",
    "        return root[1][example_attrib_val]\n",
    "    \n",
    "def predict_dataset(S, root, Label_col_index):\n",
    "    all = 0\n",
    "    correct = 0\n",
    "    hx = []\n",
    "    for idx, row in S.iterrows():\n",
    "        all += 1\n",
    "        gold_label = row[Label_col_index]\n",
    "        predicted_label = predict(root, row, Label_col_index)\n",
    "        hx.append(predicted_label)\n",
    "        if predicted_label == gold_label:\n",
    "            correct +=1\n",
    "    return correct / all , hx # accuracy, set of predictions\n",
    "        \n",
    "# ##############              ID3 implementation:\n",
    "# Input:\n",
    "# S: the set of Examples\n",
    "# Attributes: the set of measured attributes\n",
    "# Label_col_index: column index of the target attribute (the prediction)\n",
    "# max_tree_level: bounds the height of the tree\n",
    "# splitter_algorithm: can be one of the 3 values (\"ME\":Majority Error, \"GI\":Gini Index, \"EN\":Entropy)\n",
    "def ID3(S, Attributes, Label_col_index, max_tree_level, splitter_algorithm, K):\n",
    "    if(max_tree_level == 0):                                                            # if at max level\n",
    "#         print(\"max_tree_level reached\")\n",
    "        return select_label_with_max_weight_sum(S, Label_col_index)\n",
    "    elif S[Label_col_index].nunique() == 1:                                             # if all examples have same label:   \n",
    "#         print(\"Label_col_index unique\")\n",
    "        return select_label_with_max_weight_sum(S, Label_col_index)\n",
    "    elif len(Attributes) == 0:                                                          # if Attributes empty\n",
    "#         print(\"Attributes\")\n",
    "        return select_label_with_max_weight_sum(S, Label_col_index)\n",
    "    else:\n",
    "        # 1. Create a Root node for tree\n",
    "        Root = [] # each \"attribute node\" is a list s.t. \n",
    "                                                    # 1st element = string attribute name\n",
    "                                                    # 2nd element = dictionary children;\n",
    "                                                            # key = each possible attribute value v\n",
    "                                                            # value = an \"attribute node\" list;  or a string label for leaf nodes\n",
    "        # 2. A = attribute in Attributes that best splits S\n",
    "         # # randomly select k attributes\n",
    "        G = [] # holds the position of selected attributes\n",
    "        for i in range(0,K):\n",
    "            n = random.randint(0,len(Attributes)-1)\n",
    "            G.append(Attributes[n]) \n",
    "        A = Best_spliter_attribute(S, G, Label_col_index, splitter_algorithm)\n",
    "#         print(\"best is:\",attrib_name[A])\n",
    "        Root.append(attrib_name[A]) # 1st element = string attribute name\n",
    "        Root.append({})             # 2nd element = dictionary children;\n",
    "        # 3. for each possible value v of that A can take:\n",
    "        attribute_values=[]\n",
    "        if(attrib_name[A] in categorical_attrib_values):\n",
    "            attribute_values = categorical_attrib_values[attrib_name[A]]\n",
    "        else: # o.w. it is numerical \n",
    "            attribute_values = [0,1]\n",
    "        for v in attribute_values:\n",
    "            # 1. Add a new tree branch corresponding to A=v\n",
    "            # 2. Let Sv be the subset of examples in S with A=v\n",
    "            Sv = S.loc[S[A] == v]\n",
    "            if len(Sv) == 0: # if Sv is empty\n",
    "#                 print(\"Sv is empty\")\n",
    "                Root[1][v] = select_label_with_max_weight_sum(S, Label_col_index) # string label\n",
    "            else:\n",
    "                Attrib_minus_A = Attributes\n",
    "                if len(Attrib_minus_A) > 0 and A in Attrib_minus_A:\n",
    "                    Attrib_minus_A.remove(A)\n",
    "                Root[1][v] = ID3(Sv, Attrib_minus_A,Label_col_index, max_tree_level-1,splitter_algorithm, K) # an \"attribute node\" list;\n",
    "        return Root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training...\n",
      "k= 2\n",
      "total iteration: 10\n",
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "iteration: 8\n",
      "iteration: 9\n"
     ]
    }
   ],
   "source": [
    "def RandTreeLearn(S, Attributes, k): \n",
    "    tree = ID3(S, Attributes,16,16, \"EN\", k)\n",
    "    return tree\n",
    "    \n",
    "iters = 10\n",
    "Attributes = [0,1,2,3,4,5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "Trees = []\n",
    "k = 2\n",
    "print(\"Random Forest Training...\")\n",
    "print(\"k=\",k)\n",
    "print(\"total iteration:\", iters)\n",
    "for it in range(iters):\n",
    "    print(\"iteration:\", it)\n",
    "    # # generate a bootstrap tree \n",
    "    train_df_sample = train_df_processed.sample(m_train, replace=True)\n",
    "    tree = RandTreeLearn(train_df_sample, Attributes, k)\n",
    "    Trees.append(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction...\n",
      "number_of_trees: 1  ,train_Error_Rate: 0.1148 ,test_Error_Rate 0.1228\n",
      "number_of_trees: 2  ,train_Error_Rate: 0.1224 ,test_Error_Rate 0.1304\n",
      "number_of_trees: 3  ,train_Error_Rate: 0.1192 ,test_Error_Rate 0.1248\n",
      "number_of_trees: 4  ,train_Error_Rate: 0.1192 ,test_Error_Rate 0.1248\n",
      "number_of_trees: 5  ,train_Error_Rate: 0.1192 ,test_Error_Rate 0.1248\n",
      "number_of_trees: 6  ,train_Error_Rate: 0.1192 ,test_Error_Rate 0.1248\n",
      "number_of_trees: 7  ,train_Error_Rate: 0.1192 ,test_Error_Rate 0.1248\n",
      "number_of_trees: 8  ,train_Error_Rate: 0.1192 ,test_Error_Rate 0.1248\n",
      "number_of_trees: 9  ,train_Error_Rate: 0.1192 ,test_Error_Rate 0.1248\n",
      "number_of_trees: 10  ,train_Error_Rate: 0.1192 ,test_Error_Rate 0.1248\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction...\")\n",
    "for number_of_trees in range(1, 1+ len(Trees)):\n",
    "    # # take majority for predictions of all trees\n",
    "    train_prediction_frequency = pd.Series([0]*m_train)\n",
    "    test_prediction_frequency  = pd.Series([0]*m_test)\n",
    "    for t in range ( number_of_trees):\n",
    "        T = Trees[t]\n",
    "        train_accuracy, hx_train = predict_dataset(train_df_processed, T, 16)\n",
    "        test_accuracy, hx_test = predict_dataset(test_df_processed, T, 16)\n",
    "        train_prediction_frequency = train_prediction_frequency + pd.Series(hx_train)\n",
    "        test_prediction_frequency = test_prediction_frequency + pd.Series(hx_test)\n",
    "\n",
    "    final_hx_train  = train_prediction_frequency.apply(lambda x: np.sign(x))\n",
    "    final_hx_test  = test_prediction_frequency.apply(lambda x: np.sign(x))\n",
    "\n",
    "    # # compute the train and test accuracies for the final hx train and test\n",
    "    train_false_predictions = np.count_nonzero(final_hx_train - train_df[16])\n",
    "    test_false_predictions = np.count_nonzero(final_hx_test - test_df[16])\n",
    "    print(\"number_of_trees:\",number_of_trees,\" ,train_Error_Rate:\",train_false_predictions/m_train,\",test_Error_Rate\", test_false_predictions/m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
